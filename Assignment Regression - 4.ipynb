{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d1810d8",
   "metadata": {},
   "source": [
    "###### Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
    "\n",
    "Lasso Regression, short for Least Absolute Shrinkage and Selection Operator, is a regression technique known for its ability to select important features and build sparse models. This sets it apart from other regression techniques in several ways:\n",
    "\n",
    "- Key differences:\n",
    "\n",
    "1. Regularization: Like Ridge Regression, Lasso uses a penalty term to penalize large coefficients. However, unlike Ridge, which uses the sum of squares of coefficients, Lasso uses the absolute sum (L1 norm).\n",
    "\n",
    "2. Sparse models: This L1 penalty encourages setting coefficient values to zero for less informative features, resulting in models with fewer non-zero coefficients. This sparsity makes the model simpler and easier to interpret.\n",
    "\n",
    "3. Feature selection: By setting coefficients to zero, Lasso effectively acts as a feature selection method, identifying the most relevant features for the prediction task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774c1216",
   "metadata": {},
   "source": [
    "###### Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
    "\n",
    "The main advantage of using Lasso Regression in feature selection compared to other methods lies in its automatic and built-in nature. Here's why:\n",
    "\n",
    "Automatic selection: Unlike some methods that require separate feature selection steps before model building, Lasso integrates feature selection as part of its model fitting process. The L1 penalty term acts as a built-in filter, shrinking coefficients of less informative features towards zero and eventually setting them to zero altogether."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a319ac",
   "metadata": {},
   "source": [
    "###### Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
    "\n",
    "Interpreting coefficients in a Lasso Regression model is a bit more nuanced than in standard linear regression because of its key feature: sparsity. Lasso encourages simplicity by shrinking some coefficients towards zero and even setting some to zero entirely. So, let's break down the interpretation process:\n",
    "\n",
    "- Coefficient Magnitude:\n",
    "\n",
    "Sign (+/-): As in linear regression, the sign indicates the direction of the relationship. Positive means a unit increase in that feature leads to a positive change in the target variable, and vice versa.\n",
    "Absolute value: Unlike in standard regression, the magnitude doesn't directly correspond to feature importance because of shrinkage. Coefficients are shrunk towards zero based on their correlation with other features and their predictive power. A larger absolute value doesn't necessarily mean a more important feature, just that it contributes more uniquely to the prediction.\n",
    "\n",
    "- Zero Coefficients:\n",
    "\n",
    "A zero coefficient indicates the corresponding feature is excluded from the model because Lasso deemed it not statistically significant or redundant with other features. This makes Lasso useful for feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d44f93",
   "metadata": {},
   "source": [
    "###### Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?\n",
    "\n",
    "\n",
    "- Tuning parameters in Lasso Regression and their impact on performance:\n",
    "\n",
    "1. Regularization parameter (alpha or lambda):\n",
    "\n",
    "   - Controls the strength of the L1 regularization penalty\n",
    "   - Impact on performance\n",
    "   \n",
    "2. Feature scaling:\n",
    "\n",
    "   - Standardizing features\n",
    "   - Impact on performance\n",
    "   \n",
    "3.  Model selection criteria:\n",
    "\n",
    "    - Metrics like cross-validated mean squared error (MSE) or R-squared \n",
    "    - Impact on performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95869b7",
   "metadata": {},
   "source": [
    "###### Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
    "\n",
    "\n",
    "Lasso Regression is primarily designed for linear regression problems, meaning it assumes a linear relationship between the features and the target variable. However, there are ways to utilize Lasso Regression for non-linear regression problems with some modifications and caveats:\n",
    "\n",
    "1. Feature engineering:\n",
    "2. Piecewise Linear regression\n",
    "3. Kernel Methods\n",
    "4. Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1a61f2",
   "metadata": {},
   "source": [
    "###### Q6. What is the difference between Ridge Regression and Lasso Regression?\n",
    "\n",
    "\n",
    "Ridge and Lasso Regression are both regularization techniques used in linear regression to prevent overfitting and improve model generalizability. While they share the same goal, they achieve it in distinct ways, leading to key differences in their behavior and interpretation. Here's a breakdown:\n",
    "\n",
    "- Penalty Term:\n",
    "\n",
    "Ridge Regression: Uses the L2 norm penalty, which adds the sum of the squared coefficients to the cost function. This shrinks all coefficients towards zero proportionately, but doesn't necessarily set any to zero.\n",
    "Lasso Regression: Uses the L1 norm penalty, which adds the sum of the absolute values of the coefficients to the cost function. This encourages sparsity by shrinking some coefficients towards zero and setting others exactly to zero.\n",
    "\n",
    "- Feature Selection:\n",
    "\n",
    "Ridge Regression: Doesn't perform explicit feature selection, as all features remain in the model with reduced coefficients. However, it can indirectly reduce the impact of irrelevant features by shrinking their coefficients significantly.\n",
    "Lasso Regression: Can perform automatic feature selection by setting coefficients of irrelevant features to zero. This simplifies the model and enhances interpretability by identifying the most important features.\n",
    "\n",
    "- Stability:\n",
    "\n",
    "Ridge Regression: More stable when dealing with multicollinearity (correlated features). The L2 penalty penalizes the magnitude of coefficients, distributing the impact among correlated features and less likely to arbitrarily favor one over another.\n",
    "Lasso Regression: Can be less stable with severe multicollinearity. The L1 penalty's sparsity selection might arbitrarily set one correlated feature to zero, potentially impacting the model's behavior.\n",
    "\n",
    "- Bias-Variance Trade-off:\n",
    "\n",
    "Ridge Regression: Introduces a bias towards smaller coefficients, potentially underestimating the true effect of important features. However, it generally reduces variance more effectively, leading to lower prediction error.\n",
    "Lasso Regression: May introduce a bias towards simpler models due to sparsity, focusing only on highly influential features. This can increase variance if irrelevant features are not properly excluded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dbbefe",
   "metadata": {},
   "source": [
    "###### Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
    "\n",
    "Yes, Lasso Regression can handle multicollinearity in the input features to a certain extent, but it has both advantages and limitations in dealing with this challenge. Here's a breakdown:\n",
    "\n",
    "- Advantages of Lasso for Multicollinearity:\n",
    "\n",
    "Sparsity: The key strength of Lasso is its ability to set coefficients of irrelevant or redundant features to zero. In the case of multicollinearity, where features are highly correlated, Lasso might automatically remove one or more of these features, effectively reducing the impact of their redundancy on the model. This can lead to a simpler and more interpretable model that is less prone to instability due to multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714893ff",
   "metadata": {},
   "source": [
    "###### Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?\n",
    "\n",
    "Finding the optimal value of the regularization parameter (lambda) in Lasso Regression is crucial for balancing model complexity and accuracy. There's no one-size-fits-all solution, but several techniques can guide you towards the best lambda for your specific data and problem. Here are some common approaches:\n",
    "\n",
    "1. Grid Search\n",
    "2. K-fold cross validation\n",
    "3. AIC and BIC\n",
    "4. Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a785569f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
